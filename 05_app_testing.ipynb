{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"datasets/parsed_filtered_df.pkl\", \"rb\") as handle:\n",
    "    main_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df_opinion = main_df[main_df[\"sentiment\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_lang(df, langs):\n",
    "    \"\"\" filter the df with one or several languages \"\"\"\n",
    "    return df[df['lang'].isin(langs)]\n",
    "\n",
    "def filter_relevant_states(df, threshold):\n",
    "    \"\"\" keep states that have more than `threshold` tweets \"\"\"\n",
    "    return df.groupby(\"geo_state\").filter(lambda x: x.count()[\"main\"] > threshold)\n",
    "\n",
    "def append_weekday(df):\n",
    "    df[\"weekday\"] = df[\"published\"].apply(lambda x: x.weekday())\n",
    "\n",
    "def filter_weekday(df, days): #drop weekday after?\n",
    "    return df[df['weekday'].isin(days)]\n",
    "\n",
    "def filter_df(df, langs = ['en'], days = [0, 1, 2, 3, 4, 5, 6], threshold=0):\n",
    "    # Language filter\n",
    "    df = filter_lang(df, langs)\n",
    "    # Threshold filter if necessary\n",
    "    if threshold > 0:\n",
    "        df = filter_relevant_states(df, threshold)\n",
    "    # Weekday filter\n",
    "    append_weekday(df)\n",
    "    df = filter_weekday(df, days)\n",
    "    df.drop('weekday', 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bls73\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_gender</th>\n",
       "      <th>geo_state</th>\n",
       "      <th>lang</th>\n",
       "      <th>main</th>\n",
       "      <th>published</th>\n",
       "      <th>source_location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1125682</th>\n",
       "      <td>MALE</td>\n",
       "      <td>Basel-City</td>\n",
       "      <td>es</td>\n",
       "      <td>@jkn_vk bastantes menos de los que usted cree ...</td>\n",
       "      <td>2016-07-19 19:35:03</td>\n",
       "      <td>London, sometimes Switzerland, each time less ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_gender   geo_state lang  \\\n",
       "1125682          MALE  Basel-City   es   \n",
       "\n",
       "                                                      main  \\\n",
       "1125682  @jkn_vk bastantes menos de los que usted cree ...   \n",
       "\n",
       "                  published  \\\n",
       "1125682 2016-07-19 19:35:03   \n",
       "\n",
       "                                           source_location  sentiment  weekday  \n",
       "1125682  London, sometimes Switzerland, each time less ...        1.0        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df(main_df_opinion, ['es'], [0,1,2]).sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_happy_sad_tweet(df, happy):\n",
    "    if happy:\n",
    "        selected_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[0]\n",
    "    else:\n",
    "        selected_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[-1]\n",
    "    # getting indexes of tweets in the happiest/saddest state\n",
    "    indexes = main_df_opinion[main_df_opinion['geo_state'] == selected_state]['main'].index\n",
    "    # randomly selecting one of them\n",
    "    indexes_list = list(indexes)\n",
    "    random_index = int(random()*len(list(indexes)))\n",
    "    tweet_selected_index = indexes_list[random_index]\n",
    "    return main_df_opinion.loc[tweet_selected_index]['main']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'global warming is my inspiration'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_happy_sad_tweet(main_df_opinion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh wie schön könnte das Lebens ein :) heute Morgen einfach mal um CHF 13500 reicher :D #spam'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[0]\n",
    "sad_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[-1]\n",
    "\n",
    "indexes = main_df_opinion[main_df_opinion['geo_state'] == happy_state]['main'].index\n",
    "indexes_list = list(indexes)\n",
    "random_index = int(random()*len(list(indexes)))\n",
    "tweet_selected_index = indexes_list[random_index]\n",
    "main_df_opinion.loc[tweet_selected_index]['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_df(df, search_terms, search_exclusive=False):\n",
    "    \"\"\" Searches the df for either one of the terms. If `search_exclusive` is True, then searches the df for entries that have all terms \"\"\"\n",
    "    # Lowercase search terms\n",
    "    search_terms = [t.lower() for t in search_terms]\n",
    "\n",
    "    # Create a boolean array to subset the dataframe with search matching terms\n",
    "    if search_exclusive:\n",
    "        search_filter_bool = np.ones(len(df), dtype=bool)\n",
    "\n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool & df['main'].str.lower().str.contains(term)\n",
    "    else:\n",
    "        search_filter_bool = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool | df['main'].str.lower().str.contains(term)\n",
    "\n",
    "    return df[search_filter_bool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of roesti tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geo_state\n",
       "Neuchâtel           0.000022\n",
       "Ticino              0.000026\n",
       "Geneva              0.000034\n",
       "Basel-Landschaft    0.000046\n",
       "Valais              0.000048\n",
       "Basel-City          0.000059\n",
       "Schaffhausen        0.000065\n",
       "Vaud                0.000069\n",
       "Schwyz              0.000073\n",
       "Fribourg            0.000105\n",
       "Thurgau             0.000115\n",
       "Lucerne             0.000117\n",
       "Zurich              0.000136\n",
       "Aargau              0.000136\n",
       "Grisons             0.000183\n",
       "Bern                0.000246\n",
       "Obwalden            0.000262\n",
       "Saint Gallen        0.000332\n",
       "Solothurn           0.000367\n",
       "Name: main, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "roesti_search = search_df(main_df, [\"rösti\", \"roesti\", \"röschti\", \"roeschti\"], search_exclusive=False)\n",
    "roesti_tweets = roesti_search.groupby(\"geo_state\")[\"main\"].count()\n",
    "\n",
    "total_tweets = main_df.groupby(\"geo_state\")[\"main\"].count()\n",
    "(roesti_tweets / total_tweets).dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aargau</th>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Appenzell Ausserrhoden</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Appenzell Innerrhoden</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basel-City</th>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basel-Landschaft</th>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            main\n",
       "geo_state                       \n",
       "Aargau                  0.000136\n",
       "Appenzell Ausserrhoden       NaN\n",
       "Appenzell Innerrhoden        NaN\n",
       "Basel-City              0.000059\n",
       "Basel-Landschaft        0.000046"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = pd.DataFrame(roesti_tweets/total_tweets)\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_df(searched_df):\n",
    "    \"\"\"Propotion of tweets talking about a certain topic. Computation may be long. \n",
    "    `searched_df` must have been done using the whole dataset! (not only neutral)\n",
    "    Returns a dataframe\"\"\"\n",
    "    topic_tweets = searched_df.groupby(\"geo_state\")[\"main\"].count()\n",
    "    total_tweets = main_df.groupby(\"geo_state\")[\"main\"].count()\n",
    "    return pd.DataFrame(topic_tweets/total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "from random import random\n",
    "\n",
    "# ========== Tweet Selection ==========\n",
    "\n",
    "def get_happy_sad_tweet(df, happy):\n",
    "    \"\"\" Randomly selects a tweet from the happiest (`happy` = True) or saddest (`happy` = False) canton and returns it as a Serie\"\"\"\n",
    "    # If df is empty return empty string\n",
    "    if len(df) < 1:\n",
    "        return ''\n",
    "    if happy:\n",
    "        selected_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[0]\n",
    "    else:\n",
    "        selected_state = main_df_opinion.groupby(\"geo_state\").mean().sort_values(by='sentiment', ascending=False).index[-1]\n",
    "    # getting indexes of tweets in the happiest/saddest state\n",
    "    indexes = main_df_opinion[main_df_opinion['geo_state'] == selected_state]['main'].index\n",
    "    # randomly selecting one of them\n",
    "    indexes_list = list(indexes)\n",
    "    random_index = int(random()*len(list(indexes)))\n",
    "    tweet_selected_index = indexes_list[random_index]\n",
    "    return main_df_opinion.loc[tweet_selected_index]\n",
    "\n",
    "\n",
    "# ========== Sub-functions to filter data ==========\n",
    "\n",
    "def filter_lang(df, langs):\n",
    "    \"\"\" filter the df with one or several languages \"\"\"\n",
    "    return df[df['lang'].isin(langs)]\n",
    "\n",
    "def filter_relevant_states(df, threshold):\n",
    "    \"\"\" keep states that have more than `threshold` tweets \"\"\"\n",
    "    return df.groupby(\"geo_state\").filter(lambda x: x.count()[\"main\"] > threshold)\n",
    "\n",
    "def append_weekday(df):\n",
    "    df[\"weekday\"] = df[\"published\"].apply(lambda x: x.weekday())\n",
    "\n",
    "def filter_weekday(df, days): #drop weekday after?\n",
    "    return df[df['weekday'].isin(days)]\n",
    "\n",
    "def filter_df(df, langs = ['en'], days = [0, 1, 2, 3, 4, 5, 6], threshold=0):\n",
    "    \"\"\"Filters the DataFrame according to language, weekdays, and threshold\"\"\"\n",
    "    # Language filter\n",
    "    df = filter_lang(df, langs)\n",
    "    # Threshold filter if necessary\n",
    "    if threshold > 0:\n",
    "        df = filter_relevant_states(df, threshold)\n",
    "    # Weekday filter\n",
    "    append_weekday(df)\n",
    "    df = filter_weekday(df, days)\n",
    "    df = df.drop('weekday', 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========== Search & Count ==========\n",
    "\n",
    "\n",
    "def search_df(df, search_terms, search_exclusive=False):\n",
    "    \"\"\" Searches the df for either one of the terms. If `search_exclusive` is True, then searches the df for entries that have all terms \"\"\"\n",
    "    # Lowercase search terms\n",
    "    search_terms = [t.lower() for t in search_terms]\n",
    "\n",
    "    # Create a boolean array to subset the dataframe with search matching terms\n",
    "    if search_exclusive:\n",
    "        search_filter_bool = np.ones(len(df), dtype=bool)\n",
    "\n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool & df['main'].str.lower().str.contains(term)\n",
    "    else:\n",
    "        search_filter_bool = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool | df['main'].str.lower().str.contains(term)\n",
    "\n",
    "    return df[search_filter_bool]\n",
    "\n",
    "\n",
    "def count_df(searched_df, main_df):\n",
    "    \"\"\"Propotion of tweets talking about a certain topic. Computation may be long. \n",
    "    `searched_df` must have been done using the whole dataset! (not only neutral)\n",
    "    Returns a dataframe\"\"\"\n",
    "    topic_tweets = searched_df.groupby(\"geo_state\")[\"sentiment\"].count()\n",
    "    total_tweets = main_df.groupby(\"geo_state\")[\"sentiment\"].count()\n",
    "    return pd.DataFrame(topic_tweets/total_tweets)\n",
    "\n",
    "\n",
    "\n",
    "# ========== Map Generation ==========\n",
    "\n",
    "def generate_folium(df, count=False):\n",
    "    if not count:\n",
    "        df_to_map = df.groupby(\"geo_state\").mean()\n",
    "    else:\n",
    "        df_to_map = df\n",
    "    \n",
    "    df_to_map = append_state_code(df_to_map)\n",
    "\n",
    "    geo_path = 'utils/ch-cantons.topojson.json'\n",
    "\n",
    "    folium_map = folium.Map(location=[46.8, 8.2], zoom_start=8)\n",
    "    \n",
    "    print(\"if count\")\n",
    "    \n",
    "    if not count:\n",
    "        thresh = [-0.66, -0.33, 0, 0.33, 0.66]\n",
    "        colors = 'RdYlGn'\n",
    "        legend = 'Happineess level 2016 per state'\n",
    "\n",
    "    else:\n",
    "        thresh = [0, 0.01, 0.02] #try different stuff\n",
    "        colors = 'BuPu'\n",
    "        legend = 'Proportion of tweets'\n",
    "\n",
    "    folium_map.choropleth(geo_path=geo_path,\n",
    "                         data=df_to_map,\n",
    "                         columns=['state_code', 'sentiment'],\n",
    "                         key_on='feature.id',\n",
    "                         topojson='objects.cantons',\n",
    "                         threshold_scale=thresh,\n",
    "                         fill_color=colors,\n",
    "                         legend_name=legend\n",
    "                        )\n",
    "    return folium_map\n",
    "\n",
    "def append_state_code(df):\n",
    "    '''Adds state code in a new column, (semi-)in place'''\n",
    "\n",
    "    state_to_code = {\n",
    "        'Zurich': 'ZH',\n",
    "        'Solothurn': 'SO',\n",
    "        'Geneva': 'GE',\n",
    "        'Lucerne': 'LU',\n",
    "        'Thurgau': 'TG',\n",
    "        'Jura': 'JU',\n",
    "        'Grisons': 'GR',\n",
    "        'Valais': 'VS',\n",
    "        'Fribourg': 'FR',\n",
    "        'Bern': 'BE',\n",
    "        'Schaffhausen': 'SH',\n",
    "        'Schwyz': 'SZ',\n",
    "        'Vaud': 'VD',\n",
    "        'Saint Gallen': 'SG',\n",
    "        'Neuchâtel': 'NE',\n",
    "        'Aargau': 'AG',\n",
    "        'Ticino': 'TI',\n",
    "        'Basel-City': 'BS',\n",
    "        'Basel-Landschaft': 'BL',\n",
    "        'Obwalden': 'OW',\n",
    "        'Zug': 'ZG',\n",
    "        'Uri': 'UR',\n",
    "        'Glarus': 'GL',\n",
    "        'Nidwalden': 'NW',\n",
    "        'Appenzell Innerrhoden': 'AI',\n",
    "        'Appenzell Ausserrhoden': 'AR'\n",
    "    }\n",
    "\n",
    "    print(df)\n",
    "    df['state_code'] = [state_to_code[index] for index in df.index.values]\n",
    "\n",
    "    # Create missing rows to match the json\n",
    "    for state, state_code in state_to_code.items():\n",
    "        if state_code not in df['state_code'].tolist():\n",
    "            df = df.append(pd.Series({\"state_code\": state_code, \"sentiment\": 0}), ignore_index=True)\n",
    "    print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serve_map(search_query):\n",
    "\n",
    "    if search_query:\n",
    "        df = search_df(opinion_df, search_query.split(\" \"), search_exclusive=False)\n",
    "    else:\n",
    "        df = opinion_df\n",
    "\n",
    "    #df = filter_df(df, ['en'], [0, 1, 2])\n",
    "\n",
    "    print(\"yoyoyyoo\")\n",
    "    df_count = count_df(df, opinion_df)\n",
    "    #folium_map = generate_folium(df) \n",
    "    folium_map = generate_folium(df_count, count=True) \n",
    "    folium_map.save(\"app/maps/map-test-%s.html\" % search_query)\n",
    "\n",
    "    print(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoyoyyoo\n",
      "                        sentiment\n",
      "geo_state                        \n",
      "Aargau                   0.002715\n",
      "Appenzell Ausserrhoden   0.002203\n",
      "Appenzell Innerrhoden    0.001361\n",
      "Basel-City               0.000941\n",
      "Basel-Landschaft         0.003200\n",
      "Bern                     0.001683\n",
      "Fribourg                 0.003900\n",
      "Geneva                   0.002609\n",
      "Glarus                   0.000873\n",
      "Grisons                  0.002583\n",
      "Jura                     0.001718\n",
      "Lucerne                  0.001637\n",
      "Neuchâtel                0.003020\n",
      "Nidwalden                0.001661\n",
      "Obwalden                 0.005192\n",
      "Saint Gallen             0.002747\n",
      "Schaffhausen             0.002931\n",
      "Schwyz                   0.003357\n",
      "Solothurn                0.012807\n",
      "Thurgau                  0.002319\n",
      "Ticino                   0.001935\n",
      "Uri                      0.000346\n",
      "Valais                   0.002793\n",
      "Vaud                     0.004132\n",
      "Zug                      0.004159\n",
      "Zurich                   0.002348\n",
      "                        sentiment state_code\n",
      "geo_state                                   \n",
      "Aargau                   0.002715         AG\n",
      "Appenzell Ausserrhoden   0.002203         AR\n",
      "Appenzell Innerrhoden    0.001361         AI\n",
      "Basel-City               0.000941         BS\n",
      "Basel-Landschaft         0.003200         BL\n",
      "Bern                     0.001683         BE\n",
      "Fribourg                 0.003900         FR\n",
      "Geneva                   0.002609         GE\n",
      "Glarus                   0.000873         GL\n",
      "Grisons                  0.002583         GR\n",
      "Jura                     0.001718         JU\n",
      "Lucerne                  0.001637         LU\n",
      "Neuchâtel                0.003020         NE\n",
      "Nidwalden                0.001661         NW\n",
      "Obwalden                 0.005192         OW\n",
      "Saint Gallen             0.002747         SG\n",
      "Schaffhausen             0.002931         SH\n",
      "Schwyz                   0.003357         SZ\n",
      "Solothurn                0.012807         SO\n",
      "Thurgau                  0.002319         TG\n",
      "Ticino                   0.001935         TI\n",
      "Uri                      0.000346         UR\n",
      "Valais                   0.002793         VS\n",
      "Vaud                     0.004132         VD\n",
      "Zug                      0.004159         ZG\n",
      "Zurich                   0.002348         ZH\n",
      "if count\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "opinion_df = main_df_opinion\n",
    "serve_map('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
