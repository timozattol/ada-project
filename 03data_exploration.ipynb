{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import folium\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "available_paths = [\"twitter_august.pkl\",  \"twitter_july.pkl\",  \"twitter_october.pkl\",  \"twitter_september.pkl\"]\n",
    "available_paths_complete = [\"datasets/filtered_dfs/{}\".format(path) for path in available_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in available_paths_complete:\n",
    "    with open(path, \"rb\") as handle:\n",
    "        new_df = pickle.load(handle)\n",
    "        main_df = main_df.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(main_df[main_df[\"sentiment\"] == \"POSITIVE\"]))\n",
    "print(len(main_df[main_df[\"sentiment\"] == \"NEGATIVE\"]))\n",
    "print(len(main_df[main_df[\"sentiment\"] == \"NEUTRAL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_corpus = \" \".join(main_df[main_df[\"sentiment\"] == \"POSITIVE\"][\"main\"])\n",
    "negative_corpus = \" \".join(main_df[main_df[\"sentiment\"] == \"NEGATIVE\"][\"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(positive_corpus)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(negative_corpus)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.groupby(\"geo_state\").count()[\"main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing geo_state with few tweets, because they are not swiss cantons. ~0.4% of the data\n",
    "main_df = main_df.groupby(\"geo_state\").filter(lambda x: x.count()[\"main\"] > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.groupby(\"geo_state\").count()[\"main\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_cantons = [\"Baden-Württemberg\", \"Haryana\", \"North Rhine-Westphalia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing fake_cantons\n",
    "main_df = main_df[~(main_df[\"geo_state\"].isin(fake_cantons))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_clean_df(pickle_list):\n",
    "    '''Import DataFrames from different pickle files, merge them and clean the data'''\n",
    "    # == MERGE ==\n",
    "    main_df = pd.DataFrame()\n",
    "    paths = [\"datasets/filtered_dfs/{}\".format(path) for path in pickle_list]\n",
    "    \n",
    "    for path in available_paths_complete:\n",
    "        with open(path, \"rb\") as handle:\n",
    "            new_df = pickle.load(handle)\n",
    "            main_df = main_df.append(new_df)\n",
    "\n",
    "    # == CLEAN ==\n",
    "    # Remove geo_state with few tweets, because they are not swiss cantons. ~0.4% of the data\n",
    "    main_df = main_df.groupby(\"geo_state\").filter(lambda x: x.count()[\"main\"] > 1000)\n",
    "    fake_cantons = [\"Baden-Württemberg\", \"Haryana\", \"North Rhine-Westphalia\"]\n",
    "    \n",
    "    # Removing fake_cantons\n",
    "    main_df = main_df[~(main_df[\"geo_state\"].isin(fake_cantons))]\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.groupby(\"geo_state\").mean().sort_values(by=\"sentiment_int\").plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sbb_words = [\"cff\", \"sbb\", \"ffs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contains_str(string):\n",
    "    return main_df[\"main\"].str.contains(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cff_sentiment = main_df[contains_str(\"cff\") | contains_str(\"sbb\") | contains_str(\"ffs\")].groupby(\"geo_state\").mean().sort_values(by=\"sentiment_int\")\n",
    "cff_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.geo_state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_state_code(df):\n",
    "    '''Adds state code in a new column, in place'''\n",
    "    \n",
    "    state_to_code = {\n",
    "        'Zurich': 'ZH', \n",
    "        'Solothurn': 'SO', \n",
    "        'Geneva': 'GE', \n",
    "        'Lucerne': 'LU', \n",
    "        'Thurgau': 'TG', \n",
    "        'Jura': 'JU',\n",
    "        'Grisons': 'GR', \n",
    "        'Valais': 'VS', \n",
    "        'Fribourg': 'FR', \n",
    "        'Bern': 'BE', \n",
    "        'Schaffhausen': 'SH', \n",
    "        'Schwyz': 'SZ',\n",
    "        'Vaud': 'VD', \n",
    "        'Saint Gallen': 'SG', \n",
    "        'Neuchâtel': 'NE', \n",
    "        'Aargau': 'AG', \n",
    "        'Ticino': 'TI',\n",
    "        'Basel-City': 'BS', \n",
    "        'Basel-Landschaft': 'BL', \n",
    "        'Obwalden': 'OW', \n",
    "        'Zug': 'ZG', \n",
    "        'Uri': 'UR',\n",
    "        'Glarus': 'GL', \n",
    "        'Nidwalden': 'NW', \n",
    "        'Appenzell Innerrhoden': 'AI',\n",
    "        'Appenzell Ausserrhoden': 'AR'\n",
    "    }\n",
    "    \n",
    "    df['state_code'] = [state_to_code[index] for index in df.index.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "append_state_code(cff_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_path = 'utils/ch-cantons.topojson.json'\n",
    "    \n",
    "\n",
    "cff_map = folium.Map(location=[46.57, 8], zoom_start=8)\n",
    "cff_map.choropleth(geo_path=geo_path, \n",
    "                     data=cff_sentiment,\n",
    "                     columns=['state_code', 'sentiment_int'],\n",
    "                     key_on='feature.id',\n",
    "                     topojson='objects.cantons',\n",
    "                     fill_color='YlGn'\n",
    "                    )\n",
    "cff_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cff_sentiment.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sentiment = main_df.groupby(\"geo_state\").mean()\n",
    "append_state_code(all_sentiment)\n",
    "all_sentiment.sort_values(by=\"sentiment_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_path = 'utils/ch-cantons.topojson.json'\n",
    "    \n",
    "\n",
    "all_map = folium.Map(location=[46.57, 8], zoom_start=8)\n",
    "all_map.choropleth(geo_path=geo_path, \n",
    "                     data=all_sentiment,\n",
    "                     columns=['state_code', 'sentiment_int'],\n",
    "                     key_on='feature.id',\n",
    "                     topojson='objects.cantons',\n",
    "                     fill_color='YlGn'\n",
    "                    )\n",
    "all_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sentiment_en = main_df[main_df[\"lang\"] == 'en'].groupby(\"geo_state\").mean()\n",
    "append_state_code(all_sentiment_en)\n",
    "all_sentiment_en.sort_values(by=\"sentiment_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_path = 'utils/ch-cantons.topojson.json'\n",
    "    \n",
    "\n",
    "all_en_map = folium.Map(location=[46.57, 8], zoom_start=8)\n",
    "all_en_map.choropleth(geo_path=geo_path, \n",
    "                     data=all_sentiment_en,\n",
    "                     columns=['state_code', 'sentiment_int'],\n",
    "                     key_on='feature.id',\n",
    "                     topojson='objects.cantons',\n",
    "                     fill_color='YlGn'\n",
    "                    )\n",
    "all_en_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We think that the sentiment analysis works best on english, and gives lots of \"NEUTRAL\" values with other languages. It's then better to use only english (~45% of the tweets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_df(df, search_terms, search_exclusive=False, langs=[]):\n",
    "    # Lowercase search terms\n",
    "    search_terms = [t.lower() for t in search_terms]\n",
    "    \n",
    "    if len(langs) > 0:\n",
    "        lang_filtered = df[df['lang'].isin(langs)]\n",
    "    else:\n",
    "        lang_filtered = df\n",
    "    \n",
    "    # Create a boolean array to subset the dataframe with search matching terms\n",
    "    if search_exclusive:\n",
    "        search_filter_bool = np.ones(len(lang_filtered), dtype=bool)\n",
    "        \n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool & lang_filtered['main'].str.lower().str.contains(term)\n",
    "    else:\n",
    "        search_filter_bool = np.zeros(len(lang_filtered), dtype=bool)\n",
    "\n",
    "        for term in search_terms:\n",
    "            search_filter_bool = search_filter_bool | lang_filtered['main'].str.lower().str.contains(term)\n",
    "    \n",
    "    return lang_filtered[search_filter_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_df(main_df, search_terms=['sbb', 'cff', 'ffs']).sample(5)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_df(main_df, search_terms=['paleo', 'festival'], langs=['en'], search_exclusive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df.groupby(\"lang\").count().sort_values(by=\"main\", ascending=False)[\"main\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main languages are English (en), German (de), French (fr), Spanish (es) and Italian (it)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seems like there is more spanish than italian... Might be a problem with the language detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df[main_df['lang'] == 'es'].groupby('geo_state').count().sort_values('main', ascending=False)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df[main_df['lang'] == 'it'].groupby('geo_state').count().sort_values('main', ascending=False)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df[main_df['lang'] == 'pt'].groupby('geo_state').count().sort_values('main', ascending=False)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_date(df):\n",
    "    return df[\"published\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.sample()[\"published\"].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_date(string):\n",
    "    datetime_fmt = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    \n",
    "    return datetime.strptime(string, datetime_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_df.sample(10)[\"published\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df[\"published\"] = main_df[\"published\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(\"datasets/parsed_filtered_df.pkl\", \"wb\") as handle:\n",
    "#    pickle.dump(main_df, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
